import random

class Robot(object):

    def __init__(self, maze, alpha=0.5, gamma=0.9, epsilon0=0.5):
        
        # è¿·å®«å®ä½“
        self.maze = maze
        # æœ‰æ•ˆåŠ¨ä½œ
        self.valid_actions = self.maze.valid_actions
        # çŠ¶æ€
        self.state = None
        # åŠ¨ä½œ
        self.action = None

        # Set Parameters of the Learning Robot
        # ğ‘(ğ‘ ğ‘¡,ğ‘)=(1âˆ’ğ›¼)Ã—ğ‘(ğ‘ ğ‘¡,ğ‘)+ğ›¼Ã—(ğ‘…ğ‘¡+1+ğ›¾Ã—maxğ‘ğ‘(ğ‘,ğ‘ ğ‘¡+1))
        self.alpha = alpha
        self.gamma = gamma
        
        # éšæœºé€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡
        self.epsilon0 = epsilon0
        self.epsilon = epsilon0
        self.t = 0

        self.Qtable = {}
        self.reset()

    def reset(self):
        """
        Reset the robot
        é‡ç½®æ™ºèƒ½ä½“
        """
        self.state = self.sense_state()
        self.create_Qtable_line(self.state)

    def set_status(self, learning=False, testing=False):
        """
        Determine whether the robot is learning its q table, or
        exceuting the testing procedure.
        ç¡®å®šæ™ºèƒ½ä½“æ˜¯åœ¨å­¦ä¹ qè¡¨ï¼Œè¿˜æ˜¯æ‰§è¡Œæµ‹è¯•ç¨‹åºã€‚
        """
        self.learning = learning
        self.testing = testing

    def update_parameter(self):
        """
        Some of the paramters of the q learning robot can be altered,
        update these parameters when necessary.
        Q-learn æ™ºèƒ½ä½“çš„ä¸€äº›å‚æ•°å¯ä»¥æ”¹å˜,å¿…è¦æ—¶æ›´æ–°è¿™äº›å‚æ•°
        """
        if self.testing:
            # TODO 1. No random choice when testing
#             æµ‹è¯•æ—¶æ²¡æœ‰éšæœºé€‰æ‹©
            pass
        else:
            # TODO 2. Update parameters when learning
#             å­¦ä¹ æ—¶æ›´æ–°å­¦ä¹ å‚æ•°
#             æ›´æ–°æ­¥é•¿åŠéšæœºé€‰æ‹©çš„æ¦‚ç‡
            if self.epsilon < 0.01:
                self.epsilon = 0.01
            else:
                self.epsilon -= 0.05

        return self.epsilon

    def sense_state(self):
        """
        Get the current state of the robot. In this
        åœ¨è¿™é‡Œå¾—åˆ°æ™ºèƒ½ä½“çš„å½“å‰çŠ¶æ€
        """

        # TODO 3. Return robot's current state
#         maze,è¿”å›å½“å‰çŠ¶æ€
        return self.maze.sense_robot()

    def create_Qtable_line(self, state):
        """
        Create the qtable with the current state
        ä½¿ç”¨å½“å‰çŠ¶æ€åˆ›å»ºq-table
        """
        # TODO 4. Create qtable with current state
        # åˆ›å»ºqtableå¯¹äºå½“å‰çŠ¶æ€
        # Our qtable should be a two level dict,
        # æˆ‘ä»¬çš„qtableéœ€è¦æœ‰ä¸¤ä¸ªç­‰çº§çš„å­—å…¸
        # Qtable[state] ={'u':xx, 'd':xx, ...}
        # If Qtable[state] already exits, then do
        # å¦‚æœå·²ç»å­˜åœ¨,åˆ™ä¸æ”¹å˜ä»–
        # not change it.
        if state in self.Qtable:
            pass
        else:
            self.Qtable.setdefault(state, {a: 0.0 for a in self.valid_actions})

    def choose_action(self):
        """
        Return an action according to given rules
        æ ¹æ®ç»™å®šè§„åˆ™è¿”å›ä¸€ä¸ªåŠ¨ä½œ
        """
        def is_random_exploration():

            # TODO 5. Return whether do random choice
            # hint: generate a random number, and compare
            # it with epsilon
            # è¿”å›æ˜¯å¦éšæœºé€‰æ‹©ï¼Œæç¤ºï¼šç”Ÿæˆä¸€ä¸ªéšæœºæ•°ï¼Œå¹¶å°†å…¶ä¸epsilonè¿›è¡Œæ¯”è¾ƒ
            
            return random.random() < self.epsilon

        if self.learning:
            if is_random_exploration():
                # TODO 6. Return random choose aciton
                # è¿”å›éšæœºé€‰æ‹©çš„åŠ¨ä½œ
                return random.choice(self.valid_actions)
            else:
                # TODO 7. Return action with highest q value
                # è¿”å›qå€¼æœ€é«˜çš„åŠ¨ä½œ
                return max(self.Qtable[self.state], key=self.Qtable[self.state].get)
        elif self.testing:
            # TODO 7. choose action with highest q value
            # é€‰æ‹©qå€¼æœ€é«˜çš„åŠ¨ä½œ
            return max(self.Qtable[self.state], key=self.Qtable[self.state].get)
        else:
            # TODO 6. Return random choose aciton
            # é€‰æ‹©éšæœºåŠ¨ä½œ
            return random.choice(self.valid_actions)

    def update_Qtable(self, r, action, next_state):
        """
        Update the qtable according to the given rule.
        æ ¹æ®ç»™å®šçš„è§„åˆ™æ›´æ–°qtableã€‚
        
        """
        if self.learning:
            pass
            # TODO 8. When learning, update the q table according
            # to the given rules
            # æ ¹æ®ç»™å®šè§„åˆ™æ›´æ–°qè¡¨
            # ğ‘(ğ‘ ğ‘¡,ğ‘)=(1âˆ’ğ›¼)Ã—ğ‘(ğ‘ ğ‘¡,ğ‘)+ğ›¼Ã—(ğ‘…ğ‘¡+1+ğ›¾Ã—maxğ‘ğ‘(ğ‘,ğ‘ ğ‘¡+1))
            accumulate = self.Qtable[self.state][action]
            new = r + self.gamma * float(max(self.Qtable[next_state].values()))
            self.Qtable[self.state][action] += self.alpha * (new - accumulate)
    def update(self):
        """
        Describle the procedure what to do when update the robot.
        æè¿°æ›´æ–°æ™ºèƒ½ä½“æ—¶è¦æ‰§è¡Œçš„æ“ä½œæ­¥éª¤
        Called every time in every epoch in training or testing.
        åœ¨æ¯ä¸ªæ—¶é—´,åœ¨æ¯ä¸ªepoch åœ¨ å­¦ä¹ æˆ–è€…æµ‹è¯•
        Return current action and reward.
        è¿”å›å½“å‰è¡ŒåŠ¨å’Œå¥–åŠ±
        """
        # å¾—åˆ°å½“å‰çŠ¶æ€
        self.state = self.sense_state() # Get the current state
        # å¯¹äºå½“å‰çŠ¶æ€åˆ›å»ºqtableè¡Œ
        self.create_Qtable_line(self.state) # For the state, create q table line
        
        # é€‰æ‹©åŠ¨ä½œå¯¹äºå½“å‰çŠ¶æ€
        action = self.choose_action() # choose action for this state
        # ç§»åŠ¨æœºå™¨äººåˆ°ç»™å®šçš„åŠ¨ä½œ
        reward = self.maze.move_robot(action) # move robot for given action
        
        # è·å¾—ä¸‹ä¸€ä¸ªçŠ¶æ€
        next_state = self.sense_state() # get next state
        # åˆ›å»ºä¸‹ä¸€ä¸ªçŠ¶æ€çš„qtableè¡Œ
        self.create_Qtable_line(next_state) # create q table line for next state

        if self.learning and not self.testing:
            # æ›´æ–°q-table
            self.update_Qtable(reward, action, next_state) # update q table
            # æ›´æ–°å‚æ•°
            self.update_parameter() # update parameters
        # åŠ¨ä½œå¥–åŠ±
        return action, reward
